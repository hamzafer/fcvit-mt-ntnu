{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a8ab478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing TEXMET dataset...\n",
      "Input:  /cluster/home/akmarala/data/TEXMET\n",
      "Output: /cluster/home/akmarala/data/TEXMET_processed\n",
      "Max size: 2048px\n",
      "JPEG quality: 90\n",
      "--------------------------------------------------\n",
      "Processing 14915 images in train split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train split:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 11096/14915 [29:11<10:02,  6.33it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 213\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# Main Execution\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m VALIDATE_ONLY:\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;66;03m# Preprocess the dataset\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m     stats \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_texmet_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mJPEG_QUALITY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# Save processing stats\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     stats_file \u001b[38;5;241m=\u001b[39m Path(OUTPUT_DIR) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessing_stats.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[5], line 103\u001b[0m, in \u001b[0;36mpreprocess_texmet_dataset\u001b[0;34m(input_dir, output_dir, max_size, quality)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Check if image needs resizing\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m max_size \u001b[38;5;129;01mor\u001b[39;00m img\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m max_size:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# Resize maintaining aspect ratio\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthumbnail\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResampling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLANCZOS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     split_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresized\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# Track size changes\u001b[39;00m\n",
      "File \u001b[0;32m~/fcvit_env/lib/python3.8/site-packages/PIL/Image.py:2764\u001b[0m, in \u001b[0;36mImage.thumbnail\u001b[0;34m(self, size, resample, reducing_gap)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     final_size \u001b[38;5;241m=\u001b[39m preserved_size\n\u001b[1;32m   2763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m final_size:\n\u001b[0;32m-> 2764\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreducing_gap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreducing_gap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2766\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mim\n\u001b[1;32m   2767\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_size \u001b[38;5;241m=\u001b[39m final_size\n",
      "File \u001b[0;32m~/fcvit_env/lib/python3.8/site-packages/PIL/Image.py:2328\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2317\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2318\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[1;32m   2319\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2320\u001b[0m         )\n\u001b[1;32m   2321\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2322\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2323\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2324\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2325\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2326\u001b[0m         )\n\u001b[0;32m-> 2328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configuration Variables - Modify these as needed\n",
    "INPUT_DIR = \"/cluster/home/akmarala/data/TEXMET\"\n",
    "OUTPUT_DIR = \"/cluster/home/akmarala/data/TEXMET_processed\"\n",
    "MAX_SIZE = 2048\n",
    "JPEG_QUALITY = 90\n",
    "VALIDATE_ONLY = False  # Set to True to only run validation\n",
    "\n",
    "# ============================================================================\n",
    "# TEXMET Dataset Preprocessing Script\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def get_image_info(image_path):\n",
    "    \"\"\"Get image dimensions and file size\"\"\"\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            return {\n",
    "                'size': img.size,\n",
    "                'mode': img.mode,\n",
    "                'format': img.format,\n",
    "                'file_size': os.path.getsize(image_path)\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "\n",
    "def preprocess_texmet_dataset(input_dir, output_dir, max_size=2048, quality=90):\n",
    "    \"\"\"\n",
    "    Preprocess TEXMET dataset by resizing large images\n",
    "    \"\"\"\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    # Statistics tracking\n",
    "    stats = {\n",
    "        'total_processed': 0,\n",
    "        'resized_count': 0,\n",
    "        'copied_count': 0,\n",
    "        'error_count': 0,\n",
    "        'splits': {},\n",
    "        'size_distribution': {'before': {}, 'after': {}},\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    print(f\"Preprocessing TEXMET dataset...\")\n",
    "    print(f\"Input:  {input_dir}\")\n",
    "    print(f\"Output: {output_dir}\")\n",
    "    print(f\"Max size: {max_size}px\")\n",
    "    print(f\"JPEG quality: {quality}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Process each split\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_input_dir = input_path / split / 'images'\n",
    "        split_output_dir = output_path / split / 'images'\n",
    "        \n",
    "        if not split_input_dir.exists():\n",
    "            print(f\"‚ö†Ô∏è  {split} split not found, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        # Create output directory\n",
    "        split_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Get all image files\n",
    "        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}\n",
    "        image_files = []\n",
    "        for ext in image_extensions:\n",
    "            image_files.extend(list(split_input_dir.glob(f'*{ext}')))\n",
    "            image_files.extend(list(split_input_dir.glob(f'*{ext.upper()}')))\n",
    "        \n",
    "        print(f\"Processing {len(image_files)} images in {split} split...\")\n",
    "        \n",
    "        split_stats = {\n",
    "            'total': len(image_files),\n",
    "            'resized': 0,\n",
    "            'copied': 0,\n",
    "            'errors': 0\n",
    "        }\n",
    "        \n",
    "        for img_file in tqdm(image_files, desc=f\"{split} split\"):\n",
    "            try:\n",
    "                output_file = split_output_dir / f\"{img_file.stem}.jpg\"\n",
    "                \n",
    "                # Get original image info\n",
    "                original_info = get_image_info(img_file)\n",
    "                if 'error' in original_info:\n",
    "                    stats['errors'].append(f\"{img_file}: {original_info['error']}\")\n",
    "                    split_stats['errors'] += 1\n",
    "                    continue\n",
    "                \n",
    "                with Image.open(img_file) as img:\n",
    "                    img = img.convert('RGB')\n",
    "                    original_size = img.size\n",
    "                    \n",
    "                    # Check if image needs resizing\n",
    "                    if img.size[0] > max_size or img.size[1] > max_size:\n",
    "                        # Resize maintaining aspect ratio\n",
    "                        img.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)\n",
    "                        split_stats['resized'] += 1\n",
    "                        \n",
    "                        # Track size changes\n",
    "                        size_key = f\"{original_size[0]}x{original_size[1]}\"\n",
    "                        new_size_key = f\"{img.size[0]}x{img.size[1]}\"\n",
    "                        \n",
    "                        if size_key not in stats['size_distribution']['before']:\n",
    "                            stats['size_distribution']['before'][size_key] = 0\n",
    "                        stats['size_distribution']['before'][size_key] += 1\n",
    "                        \n",
    "                        if new_size_key not in stats['size_distribution']['after']:\n",
    "                            stats['size_distribution']['after'][new_size_key] = 0\n",
    "                        stats['size_distribution']['after'][new_size_key] += 1\n",
    "                        \n",
    "                    else:\n",
    "                        split_stats['copied'] += 1\n",
    "                    \n",
    "                    # Save image as JPEG\n",
    "                    img.save(output_file, 'JPEG', quality=quality, optimize=True)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_msg = f\"{img_file}: {str(e)}\"\n",
    "                stats['errors'].append(error_msg)\n",
    "                split_stats['errors'] += 1\n",
    "                print(f\"‚ùå Error: {error_msg}\")\n",
    "        \n",
    "        # Update global stats\n",
    "        stats['total_processed'] += split_stats['total']\n",
    "        stats['resized_count'] += split_stats['resized']\n",
    "        stats['copied_count'] += split_stats['copied']\n",
    "        stats['error_count'] += split_stats['errors']\n",
    "        stats['splits'][split] = split_stats\n",
    "        \n",
    "        print(f\"‚úÖ {split} split complete:\")\n",
    "        print(f\"   Total: {split_stats['total']}\")\n",
    "        print(f\"   Resized: {split_stats['resized']}\")\n",
    "        print(f\"   Copied: {split_stats['copied']}\")\n",
    "        print(f\"   Errors: {split_stats['errors']}\")\n",
    "        print()\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def validate_processed_dataset(input_dir, output_dir, max_size=2048):\n",
    "    \"\"\"\n",
    "    Validate the processed dataset\n",
    "    \"\"\"\n",
    "    print(\"üîç Validating processed dataset...\")\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    validation_results = {\n",
    "        'splits': {},\n",
    "        'size_violations': [],\n",
    "        'missing_files': [],\n",
    "        'extra_files': []\n",
    "    }\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        input_split_dir = input_path / split / 'images'\n",
    "        output_split_dir = output_path / split / 'images'\n",
    "        \n",
    "        if not input_split_dir.exists():\n",
    "            continue\n",
    "            \n",
    "        # Get file lists\n",
    "        input_files = set([f.stem for f in input_split_dir.glob('*') if f.is_file()])\n",
    "        output_files = set([f.stem for f in output_split_dir.glob('*') if f.is_file()])\n",
    "        \n",
    "        # Check for missing/extra files\n",
    "        missing = input_files - output_files\n",
    "        extra = output_files - input_files\n",
    "        \n",
    "        if missing:\n",
    "            validation_results['missing_files'].extend([f\"{split}/{f}\" for f in missing])\n",
    "        if extra:\n",
    "            validation_results['extra_files'].extend([f\"{split}/{f}\" for f in extra])\n",
    "        \n",
    "        # Check image sizes\n",
    "        oversized_images = []\n",
    "        for img_file in output_split_dir.glob('*.jpg'):\n",
    "            try:\n",
    "                with Image.open(img_file) as img:\n",
    "                    if img.size[0] > max_size or img.size[1] > max_size:\n",
    "                        oversized_images.append({\n",
    "                            'file': str(img_file),\n",
    "                            'size': img.size\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error checking {img_file}: {e}\")\n",
    "        \n",
    "        if oversized_images:\n",
    "            validation_results['size_violations'].extend(oversized_images)\n",
    "        \n",
    "        validation_results['splits'][split] = {\n",
    "            'input_count': len(input_files),\n",
    "            'output_count': len(output_files),\n",
    "            'missing_count': len(missing),\n",
    "            'extra_count': len(extra),\n",
    "            'oversized_count': len(oversized_images)\n",
    "        }\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "# ============================================================================\n",
    "# Main Execution\n",
    "# ============================================================================\n",
    "\n",
    "if not VALIDATE_ONLY:\n",
    "    # Preprocess the dataset\n",
    "    stats = preprocess_texmet_dataset(INPUT_DIR, OUTPUT_DIR, MAX_SIZE, JPEG_QUALITY)\n",
    "    \n",
    "    # Save processing stats\n",
    "    stats_file = Path(OUTPUT_DIR) / 'processing_stats.json'\n",
    "    with open(stats_file, 'w') as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    \n",
    "    print(\"üìä Processing Summary:\")\n",
    "    print(f\"Total images processed: {stats['total_processed']}\")\n",
    "    print(f\"Images resized: {stats['resized_count']}\")\n",
    "    print(f\"Images copied: {stats['copied_count']}\")\n",
    "    print(f\"Errors: {stats['error_count']}\")\n",
    "    print(f\"Stats saved to: {stats_file}\")\n",
    "    print()\n",
    "\n",
    "# Validate the processed dataset\n",
    "validation = validate_processed_dataset(INPUT_DIR, OUTPUT_DIR, MAX_SIZE)\n",
    "\n",
    "print(\"‚úÖ Validation Results:\")\n",
    "for split, results in validation['splits'].items():\n",
    "    print(f\"{split} split:\")\n",
    "    print(f\"  Input files: {results['input_count']}\")\n",
    "    print(f\"  Output files: {results['output_count']}\")\n",
    "    print(f\"  Missing files: {results['missing_count']}\")\n",
    "    print(f\"  Extra files: {results['extra_count']}\")\n",
    "    print(f\"  Oversized images: {results['oversized_count']}\")\n",
    "    \n",
    "    if results['input_count'] == results['output_count']:\n",
    "        print(f\"  ‚úÖ File count matches!\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå File count mismatch!\")\n",
    "\n",
    "# Report any issues\n",
    "if validation['missing_files']:\n",
    "    print(f\"\\n‚ùå Missing files ({len(validation['missing_files'])}):\")\n",
    "    for f in validation['missing_files'][:10]:  # Show first 10\n",
    "        print(f\"  {f}\")\n",
    "    if len(validation['missing_files']) > 10:\n",
    "        print(f\"  ... and {len(validation['missing_files']) - 10} more\")\n",
    "\n",
    "if validation['extra_files']:\n",
    "    print(f\"\\n‚ö†Ô∏è  Extra files ({len(validation['extra_files'])}):\")\n",
    "    for f in validation['extra_files'][:10]:\n",
    "        print(f\"  {f}\")\n",
    "    if len(validation['extra_files']) > 10:\n",
    "        print(f\"  ... and {len(validation['extra_files']) - 10} more\")\n",
    "\n",
    "if validation['size_violations']:\n",
    "    print(f\"\\n‚ùå Oversized images ({len(validation['size_violations'])}):\")\n",
    "    for img in validation['size_violations'][:5]:\n",
    "        print(f\"  {img['file']}: {img['size']}\")\n",
    "    if len(validation['size_violations']) > 5:\n",
    "        print(f\"  ... and {len(validation['size_violations']) - 5} more\")\n",
    "\n",
    "if (not validation['missing_files'] and \n",
    "    not validation['extra_files'] and \n",
    "    not validation['size_violations']):\n",
    "    print(\"\\nüéâ Dataset preprocessing completed successfully!\")\n",
    "    print(\"All files processed, no size violations, file counts match!\")\n",
    "    print(f\"\\nYou can now use the processed dataset:\")\n",
    "    print(f\"--data_path {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
